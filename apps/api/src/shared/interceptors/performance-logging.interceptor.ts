import {
  Injectable,
  NestInterceptor,
  ExecutionContext,
  CallHandler,
  Logger,
} from '@nestjs/common';
import { Observable } from 'rxjs';
import { tap } from 'rxjs/operators';

/**
 * PerformanceLoggingInterceptor - Monitoreo de Latencia (PASO 3.4)
 *
 * OBJETIVO:
 * Medir y loggear autom√°ticamente la latencia de todos los endpoints HTTP
 * para identificar bottlenecks y degradaci√≥n de performance.
 *
 * M√âTRICAS CAPTURADAS:
 * - Latencia total del request (ms)
 * - M√©todo HTTP (GET, POST, etc.)
 * - Ruta del endpoint
 * - Status code de respuesta
 * - Timestamp del request
 *
 * USO:
 * Se aplica globalmente en main.ts con app.useGlobalInterceptors()
 * o se puede aplicar a nivel de controller con @UseInterceptors()
 *
 * FORMATO DE LOG:
 * [Performance] GET /api/inscripciones-2026 - 245ms - 200
 *
 * ALERTAS AUTOM√ÅTICAS:
 * - WARN si latencia > 1000ms (endpoints lentos)
 * - ERROR si latencia > 3000ms (endpoints cr√≠ticos)
 *
 * @example
 * // En main.ts (aplicaci√≥n global)
 * app.useGlobalInterceptors(new PerformanceLoggingInterceptor());
 *
 * // En controller espec√≠fico
 * @UseInterceptors(PerformanceLoggingInterceptor)
 * @Controller('inscripciones-2026')
 * export class Inscripciones2026Controller { ... }
 */
@Injectable()
export class PerformanceLoggingInterceptor implements NestInterceptor {
  private readonly logger = new Logger('PerformanceMonitor');

  // Thresholds de latencia (en ms)
  private readonly SLOW_THRESHOLD = 1000; // 1 segundo
  private readonly CRITICAL_THRESHOLD = 3000; // 3 segundos

  intercept(context: ExecutionContext, next: CallHandler): Observable<any> {
    const request = context.switchToHttp().getRequest();
    const { method, url } = request;
    const startTime = Date.now();

    return next.handle().pipe(
      tap({
        next: () => {
          this.logPerformance(context, startTime, method, url);
        },
        error: (error) => {
          this.logPerformance(context, startTime, method, url, error);
        },
      }),
    );
  }

  private logPerformance(
    context: ExecutionContext,
    startTime: number,
    method: string,
    url: string,
    error?: any,
  ): void {
    const response = context.switchToHttp().getResponse();
    const latency = Date.now() - startTime;
    const statusCode = error ? error.status || 500 : response.statusCode;

    // Construir mensaje de log
    const logMessage = `${method} ${url} - ${latency}ms - ${statusCode}`;

    // Loggear seg√∫n threshold de latencia
    if (error) {
      this.logger.error(`‚ùå ${logMessage} - Error: ${error.message}`);
    } else if (latency > this.CRITICAL_THRESHOLD) {
      this.logger.error(`üî¥ CRITICAL LATENCY: ${logMessage}`);
    } else if (latency > this.SLOW_THRESHOLD) {
      this.logger.warn(`‚ö†Ô∏è SLOW REQUEST: ${logMessage}`);
    } else {
      this.logger.log(`‚úÖ ${logMessage}`);
    }

    // M√©tricas estructuradas (para integraci√≥n con monitoring tools como Datadog, New Relic, etc.)
    this.emitMetrics({
      type: 'http_request',
      method,
      url,
      latency,
      statusCode,
      timestamp: new Date().toISOString(),
      error: error?.message,
    });
  }

  /**
   * Emite m√©tricas en formato estructurado para herramientas de monitoring
   * En producci√≥n, esto se puede integrar con:
   * - Datadog APM
   * - New Relic
   * - Prometheus
   * - CloudWatch (AWS)
   */
  private emitMetrics(metrics: {
    type: string;
    method: string;
    url: string;
    latency: number;
    statusCode: number;
    timestamp: string;
    error?: string;
  }): void {
    // Por ahora solo loggeamos en formato JSON
    // En producci√≥n, aqu√≠ ir√≠a la integraci√≥n con el servicio de monitoring
    if (process.env.NODE_ENV === 'production') {
      // Ejemplo: datadog.increment('http.requests', 1, tags);
      // Ejemplo: newrelic.recordMetric('Custom/Latency', metrics.latency);
      this.logger.debug(JSON.stringify(metrics));
    }
  }
}
